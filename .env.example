# DCI Research Agent System — Environment Variables
# Copy this file to .env and fill in your values

# ── SLM Inference APIs ──────────────────────────────────────────────
# At least one inference provider is required for agent reasoning.
# Groq is recommended (free tier available at console.groq.com).
GROQ_API_KEY=
TOGETHER_API_KEY=
FIREWORKS_API_KEY=

# ── Self-Hosted Inference (Ollama) ──────────────────────────────────
# If running SLMs locally via Ollama, set the base URL.
# Default: http://localhost:11434
OLLAMA_BASE_URL=http://localhost:11434

# ── Knowledge Graph (Neo4j) ─────────────────────────────────────────
# Neo4j Community Edition connection details.
# Local Docker: bolt://localhost:7687
# AuraDB free tier: neo4j+s://<id>.databases.neo4j.io
NEO4J_URI=bolt://localhost:7687
NEO4J_USERNAME=neo4j
NEO4J_PASSWORD=password

# ── Model Selection ─────────────────────────────────────────────────
# Override default model assignments per agent.
# See config/constants.py for defaults.
# ROUTER_MODEL=gemma3:1b
# DOMAIN_MODEL=qwen3:4b
# MATH_MODEL=deepseek-r1-distill-qwen-7b
# CODE_MODEL=qwen3:8b
# SYNTHESIS_MODEL=qwen3:8b
# CRITIQUE_MODEL=phi4-mini-reasoning

# ── Inference Provider Priority ─────────────────────────────────────
# Comma-separated priority order. System tries each in order.
# Options: ollama, groq, together, fireworks
INFERENCE_PRIORITY=groq,together,fireworks,ollama

# ── Application Settings ────────────────────────────────────────────
LOG_LEVEL=INFO
STREAMLIT_PORT=8501
API_PORT=8000
